### 模型在预训练还是在指令微调阶段进行LoRA训练？

本项目在预训练和指令微调阶段均使用LoRA进行高效训练。

### 为什么对模型进行LoRA训练而非全量参数训练？

考虑到训练成本和效率等因素，我们选择在Alpaca-2的基础上使用LoRA进行训练（embedding/lm_head全量参与训练）。

### 为什么选择在Alpaca-2的基础上进行训练，而非Llama-2？

Llama-2虽然已具备一定的中文理解能力，但在生成中文文本时仍然会夹杂英文，且中文词表大小有限，需要进行进一步的中文能力扩展，Alpaca-2在该方面已经做出了出色的工作。
